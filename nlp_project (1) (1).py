# -*- coding: utf-8 -*-
"""NLP project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OFUy88guTXfs760zYTHf8j8INNVDIZJt
"""

pip install transformers datasets torch accelerate sentencepiece

pip install opencv-python

!pip install evaluate

import torch
import pandas as pd
from datasets import load_dataset
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, Trainer, TrainingArguments, DataCollatorForSeq2Seq

# ‚úÖ Load tokenizer and model
model_name = "facebook/mbart-large-50"
tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
model = MBartForConditionalGeneration.from_pretrained(model_name)

# ‚úÖ Set language (Use 'mr_IN' for Marathi)
tokenizer.src_lang = "mr_IN"
tokenizer.tgt_lang = "mr_IN"

print("‚úÖ Tokenizer and model loaded successfully!")

# ‚úÖ Load dataset efficiently
data_files = {"train": "/content/marathi_output_train (3).csv", "test": "/content/marathi_output_test.csv"}
dataset = load_dataset("csv", data_files=data_files)

# ‚úÖ Tokenization function
def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["Text"],
        max_length=512,
        truncation=True,
        padding="max_length"
    )
    labels = tokenizer(
        examples["Summary"],
        max_length=128,
        truncation=True,
        padding="max_length"
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# ‚úÖ Apply tokenization
tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=["Title", "Summary", "Text"])

# ‚úÖ Move model to GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# ‚úÖ Training arguments (Optimized for GPU)
training_args = TrainingArguments(
    output_dir="./mbart_marathi_summarization",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=5,
    logging_dir="./logs",
    logging_steps=10,  # ‚úÖ Log every 10 steps
    report_to="none",
    save_steps=1000,
    load_best_model_at_end=True,
    push_to_hub=False,
    fp16=True,  # ‚úÖ Enable FP16 for faster GPU training
    disable_tqdm=False,  # ‚úÖ Ensure progress bar is visible
)

# ‚úÖ Data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# ‚úÖ Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator
)

# ‚úÖ Train the model on GPU
if __name__ == "__main__":
    print("üî• Training started on GPU...")
    train_results = trainer.train()
    print("‚úÖ Training completed!")

    # ‚úÖ Print training metrics
    print(train_results)

    # ‚úÖ Evaluate the model
    eval_results = trainer.evaluate()
    print(f"üìä Evaluation results:\n{eval_results}")

    # ‚úÖ Save trained model
    model.save_pretrained("./mbart_marathi_summarization")
    tokenizer.save_pretrained("./mbart_marathi_summarization")
    print("üìÅ Model and tokenizer saved successfully!")

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

# Load trained model
model_name = "/content/mbart_marathi_summarization"
tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
model = MBartForConditionalGeneration.from_pretrained(model_name).to("cuda")

# Input text for summarization
input_text = "2014 ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§î‡§∞‡§Ç‡§ó‡§æ‡§¨‡§æ‡§¶ ‡§Æ‡§ß‡•ç‡§Ø ‡§Ü‡§£‡§ø ‡§Æ‡•Å‡§Ç‡§¨‡§à‡§§‡•Ä‡§≤ ‡§≠‡§æ‡§Ø‡§ñ‡§≥‡§æ ‡§Ø‡§æ ‡§¶‡•ã‡§® ‡§Æ‡§§‡§¶‡§æ‡§∞‡§∏‡§Ç‡§ò‡§æ‡§§‡•Ç‡§® ‡§è‡§Æ‡§Ü‡§Ø‡§è‡§Æ‡§ö‡•á ‡§Ü‡§Æ‡§¶‡§æ‡§∞ ‡§®‡§ø‡§µ‡§°‡•Ç‡§® ‡§Ü‡§≤‡•á ‡§π‡•ã‡§§‡•á. ‡§Æ‡§æ‡§§‡•ç‡§∞, ‡§Ø‡§Ç‡§¶‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡•ç‡§π‡§£‡§ú‡•á 2019 ‡§ö‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ ‡§®‡§ø‡§µ‡§°‡§£‡•Å‡§ï‡•Ä‡§§ ‡§è‡§Æ‡§Ü‡§Ø‡§è‡§Æ‡§®‡§Ç ‡§Ø‡§æ ‡§¶‡•ã‡§®‡•ç‡§π‡•Ä ‡§ú‡§æ‡§ó‡§æ ‡§ó‡§Æ‡§æ‡§µ‡§≤‡•ç‡§Ø‡§æ ‡§Ü‡§π‡•á‡§§. ‡§Æ‡§æ‡§§‡•ç‡§∞, ‡§è‡§Æ‡§Ü‡§Ø‡§è‡§Æ‡§ö‡•Ä ‡§Ü‡§Æ‡§¶‡§æ‡§∞‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§¶‡•ã‡§®‡§ö ‡§∞‡§æ‡§π‡§ø‡§≤‡•Ä ‡§Ü‡§π‡•á. ‡§ï‡§æ‡§∞‡§£ ‡§ß‡•Å‡§≥‡•á ‡§∂‡§π‡§∞ ‡§Ü‡§£‡§ø ‡§Æ‡§æ‡§≤‡•á‡§ó‡§æ‡§µ ‡§Æ‡§ß‡•ç‡§Ø ‡§Ø‡§æ ‡§¶‡•ã‡§® ‡§ú‡§æ‡§ó‡§æ ‡§ú‡§ø‡§Ç‡§ï‡§£‡•ç‡§Ø‡§æ‡§§ ‡§è‡§Æ‡§Ü‡§Ø‡§è‡§Æ‡§≤‡§æ ‡§Ø‡§∂ ‡§Æ‡§ø‡§≥‡§æ‡§≤‡§Ç ‡§Ü‡§π‡•á. ‡§ß‡•Å‡§≥‡•á ‡§∂‡§π‡§∞ ‡§Æ‡§§‡§¶‡§æ‡§∞‡§∏‡§Ç‡§ò‡§æ‡§§‡•Ç‡§® ‡§´‡§æ‡§∞‡•Ç‡§ï ‡§∂‡§æ‡§π ‡§§‡§∞ ‡§Æ‡§æ‡§≤‡•á‡§ó‡§æ‡§µ ‡§Æ‡§ß‡•ç‡§Ø ‡§Æ‡§§‡§¶‡§æ‡§∞‡§∏‡§Ç‡§ò‡§æ‡§§‡•Ç‡§® ‡§Æ‡•Å‡§´‡•ç‡§§‡•Ä ‡§Æ‡•ã‡§π‡§Æ‡•ç‡§Æ‡§¶ ‡§á‡§∏‡•ç‡§Æ‡§æ‡§à‡§≤ ‡§ú‡§ø‡§Ç‡§ï‡§≤‡•á ‡§Ü‡§π‡•á‡§§. ‡§Ø‡§Ç‡§¶‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§æ‡§§ ‡§ï‡•ã‡§£ ‡§ï‡§ø‡§§‡•Ä ‡§ú‡§æ‡§ó‡§æ ‡§ú‡§ø‡§Ç‡§ï‡§≤‡§Ç? ‡§î‡§∞‡§Ç‡§ó‡§æ‡§¨‡§æ‡§¶ ‡§Æ‡§ß‡•ç‡§Ø ‡§Ü‡§£‡§ø ‡§Æ‡•Å‡§Ç‡§¨‡§à‡§§‡•Ä‡§≤ ‡§≠‡§æ‡§Ø‡§ñ‡§≥‡§æ ‡§Ø‡§æ ‡§¶‡•ã‡§® ‡§Æ‡§§‡§¶‡§æ‡§∞‡§∏‡§Ç‡§ò‡§æ‡§§‡•Ç‡§® 2014 ‡§∏‡§æ‡§≤‡•Ä ‡§è‡§Æ‡§Ü‡§Ø‡§è‡§Æ‡§ö‡•á ‡§Ü‡§Æ‡§¶‡§æ‡§∞ ‡§ú‡§ø‡§Ç‡§ï‡§≤‡•á ‡§π‡•ã‡§§‡•á. ‡§π‡•Ä ‡§¶‡•ã‡§®‡•ç‡§π‡•Ä ‡§†‡§ø‡§ï‡§æ‡§£‡§Ç ‡§Æ‡•Å‡§∏‡•ç‡§≤‡•Ä‡§Æ‡§¨‡§π‡•Å‡§≤ ‡§≤‡•ã‡§ï‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•Ä ‡§Ü‡§π‡•á‡§§. ‡§Æ‡§æ‡§§‡•ç‡§∞, ‡§Ø‡§Ç‡§¶‡§æ ‡§ú‡§ø‡§Ç‡§ï‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§Æ‡§æ‡§≤‡•á‡§ó‡§æ‡§µ ‡§Æ‡§ß‡•ç‡§Ø ‡§µ‡§ó‡§≥‡§≤‡•ç‡§Ø‡§æ‡§∏ ‡§ß‡•Å‡§≥‡•á ‡§∂‡§π‡§∞‡§æ‡§§ ‡§®‡§ø‡§∞‡•ç‡§£‡§æ‡§Ø‡§ï ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§Æ‡§§‡§Ç ‡§®‡§æ‡§π‡•Ä‡§§. ‡§§‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§ß‡•Å‡§≥‡•á ‡§∂‡§π‡§∞‡§æ‡§§ ‡§è‡§Æ‡§Ü‡§Ø‡§è‡§Æ‡§®‡§Ç ‡§ï‡§∂‡•Ä ‡§¨‡§æ‡§ú‡•Ä ‡§Æ‡§æ‡§∞‡§≤‡•Ä ‡§Ü‡§£‡§ø ‡§ï‡•ã‡§£‡§§‡•Ä ‡§∏‡§Æ‡•Ä‡§ï‡§∞‡§£‡§Ç ‡§ï‡§æ‡§Æ‡•Ä ‡§Ü‡§≤‡•Ä, ‡§Ø‡§æ‡§ö‡§æ ‡§Ü‡§¢‡§æ‡§µ‡§æ ‡§¨‡•Ä‡§¨‡•Ä‡§∏‡•Ä ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§®‡§Ç ‡§ò‡•á‡§§‡§≤‡§æ ‡§Ü‡§π‡•á. ‡§ß‡•Å‡§≥‡•á ‡§∂‡§π‡§∞‡§æ‡§§ ‡§è‡§Æ‡§Ü‡§Ø‡§è‡§Æ‡§ö‡§æ ‡§µ‡§ø‡§ú‡§Ø ‡§ï‡§∏‡§æ ‡§ù‡§æ‡§≤‡§æ? ‡§ß‡•Å‡§≥‡•á ‡§Æ‡§π‡§æ‡§™‡§æ‡§≤‡§ø‡§ï‡§æ ‡§®‡§ø‡§µ‡§°‡§£‡•Å‡§ï‡•Ä‡§™‡§æ‡§∏‡•Ç‡§®‡§ö ‡§≠‡§æ‡§ú‡§™‡§ö‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§Ü‡§Æ‡§¶‡§æ‡§∞ ‡§Ö‡§®‡§ø‡§≤ ‡§ó‡•ã‡§ü‡•á ‡§™‡§ï‡•ç‡§∑‡§æ‡§µ‡§∞ ‡§®‡§æ‡§∞‡§æ‡§ú ‡§π‡•ã‡§§‡•á. ‡§§‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡§Ç ‡§§‡•á ‡§ß‡•Å‡§≥‡•á ‡§∂‡§π‡§∞‡§æ‡§§‡•Ç‡§® ‡§Ö‡§™‡§ï‡•ç‡§∑ ‡§≤‡§¢‡§≤‡•á. ‡§ï‡§æ‡§Å‡§ó‡•ç‡§∞‡•á‡§∏-‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§µ‡§æ‡§¶‡•Ä‡§®‡§Ç ‡§á‡§•‡§Ç ‡§â‡§Æ‡•á‡§¶‡§µ‡§æ‡§∞ ‡§® ‡§¶‡•á‡§§‡§æ ‡§Ö‡§®‡§ø‡§≤ ‡§ó‡•ã‡§ü‡•á‡§Ç‡§®‡§æ ‡§™‡§æ‡§†‡§ø‡§Ç‡§¨‡§æ ‡§¶‡§ø‡§≤‡§æ ‡§π‡•ã‡§§‡§æ. ‡§Ü‡§ò‡§æ‡§°‡•Ä‡§®‡§Ç ‡§ó‡•ã‡§ü‡•á‡§Ç‡§®‡§æ ‡§™‡§æ‡§†‡§ø‡§Ç‡§¨‡§æ ‡§¶‡§ø‡§≤‡•ç‡§Ø‡§æ‡§®‡§Ç ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§µ‡§æ‡§¶‡•Ä‡§ö‡•á ‡§Æ‡§æ‡§ú‡•Ä ‡§Ü‡§Æ‡§¶‡§æ‡§∞ ‡§∞‡§æ‡§ú‡§µ‡§∞‡•ç‡§ß‡§® ‡§ï‡§¶‡§Æ‡§¨‡§æ‡§Ç‡§°‡•á ‡§®‡§æ‡§∞‡§æ‡§ú ‡§ù‡§æ‡§≤‡•á ‡§Ü‡§£‡§ø ‡§Ö‡§™‡§ï‡•ç‡§∑ ‡§Æ‡•ç‡§π‡§£‡•Ç‡§® ‡§∞‡§ø‡§Ç‡§ó‡§£‡§æ‡§§ ‡§â‡§§‡§∞‡§≤‡•á. ‡§§‡•ç‡§Ø‡§æ‡§§ ‡§∂‡§ø‡§µ‡§∏‡•á‡§®‡•á‡§®‡§Ç‡§π‡•Ä ‡§π‡§ø‡§≤‡§æ‡§≤ ‡§≤‡§æ‡§≤‡§æ ‡§Æ‡§æ‡§≥‡•Ä‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§∞‡•Ç‡§™‡§æ‡§®‡§Ç ‡§â‡§Æ‡•á‡§¶‡§µ‡§æ‡§∞ ‡§¶‡§ø‡§≤‡§æ ‡§π‡•ã‡§§‡§æ. ‡§ß‡•Å‡§≥‡•á ‡§∂‡§π‡§∞ ‡§Æ‡§§‡§¶‡§æ‡§∞‡§∏‡§Ç‡§ò‡§æ‡§§ ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ‡§æ‡§Ç‡§ö‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§≤‡§ï‡•ç‡§∑‡§£‡•Ä‡§Ø ‡§Ü‡§π‡•á. ‡§§‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡§Ç ‡§è‡§Æ‡§Ü‡§Ø‡§è‡§Æ‡§®‡§Ç‡§π‡•Ä ‡§Ø‡§æ ‡§Æ‡§§‡§¶‡§æ‡§∞‡§∏‡§Ç‡§ò‡§æ‡§§ ‡§≤‡§ï‡•ç‡§∑ ‡§¶‡•á‡§§ ‡§´‡§æ‡§∞‡•Ç‡§ï ‡§∂‡§æ‡§π ‡§Ø‡§æ‡§Ç‡§®‡§æ ‡§∞‡§ø‡§Ç‡§ó‡§£‡§æ‡§§ ‡§â‡§§‡§∞‡§µ‡§≤‡§Ç ‡§π‡•ã‡§§‡§Ç. ‡§´‡§æ‡§∞‡•Ç‡§ï ‡§∂‡§æ‡§π ‡§ß‡•Å‡§≥‡•á ‡§∂‡§π‡§∞ ‡§Æ‡§§‡§¶‡§æ‡§∞‡§∏‡§Ç‡§ò‡§æ‡§ö‡§æ ‡§ó‡•á‡§≤‡•ç‡§Ø‡§æ ‡§¶‡•ã‡§® ‡§¶‡§∂‡§ï‡§æ‡§Ç‡§ö‡§æ ‡§á‡§§‡§ø‡§π‡§æ‡§∏ ‡§™‡§æ‡§π‡§§‡§æ ‡§π‡§æ ‡§Æ‡§§‡§¶‡§æ‡§∞‡§∏‡§Ç‡§ò 1999 ‡§∏‡§æ‡§≤‡§æ‡§™‡§æ‡§∏‡•Ç‡§® ‡§Ü‡§≤‡§ü‡•Ç‡§®-‡§™‡§æ‡§≤‡§ü‡•Ç‡§® ‡§Ö‡§®‡§ø‡§≤ ‡§ó‡•ã‡§ü‡•á ‡§Ü‡§£‡§ø ‡§∞‡§æ‡§ú‡§µ‡§∞‡•ç‡§ß‡§® ‡§ï‡§Æ‡§¶‡§¨‡§æ‡§Ç‡§°‡•á ‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ‡§ï‡§°‡•á‡§ö ‡§∞‡§æ‡§π‡§ø‡§≤‡§æ ‡§Ü‡§π‡•á. 1995 ‡§Ü‡§£‡§ø 2004 ‡§Ö‡§∂‡§æ ‡§¶‡•ã‡§®‡§µ‡•á‡§≥‡§æ ‡§∞‡§æ‡§ú‡§µ‡§∞‡•ç‡§ß‡§® ‡§ï‡§¶‡§Æ‡§¨‡§æ‡§Ç‡§°‡•á ‡§§‡§∞ 1999, 2009 ‡§Ü‡§£‡§ø 2014 ‡§Ö‡§∂‡§æ ‡§§‡•Ä‡§®‡§µ‡•á‡§≥‡§æ ‡§Ö‡§®‡§ø‡§≤ ‡§ó‡•ã‡§ü‡•á ‡§á‡§•‡•Ç‡§® ‡§µ‡§ø‡§ú‡§Ø‡•Ä ‡§ù‡§æ‡§≤‡•á ‡§Ü‡§π‡•á‡§§. ‡§ó‡•á‡§≤‡•Ä ‡§¶‡•ã‡§® ‡§¶‡§∂‡§ï‡§Ç ‡§Ü‡§≤‡§ü‡•Ç‡§®-‡§™‡§æ‡§≤‡§ü‡•Ç‡§® ‡§ß‡•Å‡§≥‡•á ‡§∂‡§π‡§∞‡§æ‡§ö‡§Ç ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡•á‡§§ ‡§™‡•ç‡§∞‡§§‡§ø‡§®‡§ø‡§ß‡§ø‡§§‡•ç‡§µ ‡§ï‡§∞‡§£‡§æ‡§±‡•ç‡§Ø‡§æ ‡§Ö‡§®‡§ø‡§≤ ‡§ó‡•ã‡§ü‡•á ‡§Ü‡§£‡§ø ‡§∞‡§æ‡§ú‡§µ‡§∞‡•ç‡§ß‡§® ‡§ï‡§¶‡§Æ‡§¨‡§æ‡§Ç‡§°‡•á ‡§Ø‡§æ‡§Ç‡§®‡§æ ‡§¨‡§æ‡§ú‡•Ç‡§≤‡§æ ‡§∏‡§æ‡§∞‡§§ ‡§á‡§•‡§≤‡•ç‡§Ø‡§æ ‡§ú‡§®‡§§‡•á‡§®‡§Ç ‡§è‡§Æ‡§Ü‡§Ø‡§è‡§Æ‡§ö‡•á ‡§´‡§æ‡§∞‡•Ç‡§ï ‡§∂‡§æ‡§π ‡§Ø‡§æ‡§Ç‡§®‡§æ ‡§®‡§ø‡§µ‡§°‡§≤‡§Ç‡§Ø. ‡§â‡§§‡•ç‡§§‡§∞ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§æ‡§§‡•Ä‡§≤ ‡§µ‡§∞‡§ø‡§∑‡•ç‡§† ‡§™‡§§‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§Æ‡§ø‡§≤‡§ø‡§Ç‡§¶ ‡§∏‡§ú‡§ó‡•Å‡§∞‡•á ‡§∏‡§æ‡§Ç‡§ó‡§§‡§æ‡§§"

# Tokenize input
inputs = tokenizer(input_text, return_tensors="pt", truncation=True).to("cuda")

# Generate summary
summary_ids = model.generate(
    **inputs,
    max_length=128,
    num_beams=5,
    early_stopping=True
)

summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print("Generated Summary:", summary_text)

pip install datasets rouge_score

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
import torch

# Load fine-tuned model
model_path = "/content/mbart_marathi_summarization"  # Update this path if needed
tokenizer = MBart50TokenizerFast.from_pretrained(model_path)
model = MBartForConditionalGeneration.from_pretrained(model_path).to("cuda")

# Set tokenizer source language to Marathi
tokenizer.src_lang = "mr_IN"

import pandas as pd

# Load test dataset
test_data_path = "/content/marathi_output_test.csv"  # Update path if needed
df = pd.read_csv(test_data_path)

# Extract text and corresponding reference summaries
texts = df["Text"].tolist()
references = df["Summary"].tolist()

print(f"Loaded {len(texts)} test samples.")

def generate_summary(text):
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True).to("cuda")

    # Force the output to Marathi
    forced_bos_token_id = tokenizer.lang_code_to_id["mr_IN"]

    summary_ids = model.generate(
        **inputs,
        max_length=100,
        num_beams=5,
        forced_bos_token_id=forced_bos_token_id
    )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Generate summaries for all test samples
predictions = [generate_summary(text) for text in texts]

# Print a few examples
for i in range(3):  # Show first 3 summaries
    print(f"\nOriginal: {texts[i]}")
    print(f"Reference Summary: {references[i]}")
    print(f"Generated Summary: {predictions[i]}")

!pip install evaluate  # Install the new evaluate library

from evaluate import load

# Load ROUGE metric
rouge = load("rouge")

# Compute ROUGE scores
rouge_scores = rouge.compute(predictions=predictions, references=references)

# Print formatted results
for key, value in rouge_scores.items():
    print(f"{key}: {value:.4f}")

import shutil

# Path where your model is saved (update if needed)
model_path = "/content/mbart_marathi_summarization"

# Create a zip file of the model directory
shutil.make_archive("mbart_marathi_summarization", 'zip', model_path)

print("Model zipped successfully!")